{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e9d898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO    :__main__.initialize_dir_structure>> creating subdirs\n",
      "INFO    :__main__.initialize_dir_structure>> creating dailythanthi2\n",
      "INFO    :__main__.mkdir   >> creating dailythanthi2\n",
      "INFO    :__main__.initialize_dir_structure>> creating dailythanthi2/articles\n",
      "INFO    :__main__.mkdir   >> creating dailythanthi2/articles\n",
      "INFO    :__main__.initialize_dir_structure>> creating dailythanthi2/comments\n",
      "INFO    :__main__.mkdir   >> creating dailythanthi2/comments\n",
      "INFO    :__main__.initialize_dir_structure>> creating dailythanthi2/abstracts\n",
      "INFO    :__main__.mkdir   >> creating dailythanthi2/abstracts\n",
      "INFO    :__main__.<module>>> crawl_count: 0\n",
      "INFO    :__main__.<module>>> processing: http://www.dailythanthi.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import urllib\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "from pprint import pprint, pformat\n",
    "FORMAT_STRING = \"%(levelname)-8s:%(name)-8s.%(funcName)-8s>> %(message)s\"\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format=FORMAT_STRING)\n",
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "MAX_COUNT = 1000000000000\n",
    "\n",
    "ROOT_DIR = 'dailythanthi2'\n",
    "\n",
    "LINKS_FILEPATH         = '{}/links.list'.format(ROOT_DIR)\n",
    "VISITED_LINKS_FILEPATH = '{}/visited-links.list'.format(ROOT_DIR)\n",
    "\n",
    "TITLE_LIST_FILEPATH    = '{}/title.list'        .format(ROOT_DIR)\n",
    "ARTICLES_DIR           = '{}/articles'          .format(ROOT_DIR)\n",
    "ABSTRACTS_DIR          = '{}/abstracts'         .format(ROOT_DIR)\n",
    "COMMENTS_DIR           = '{}/comments'          .format(ROOT_DIR)\n",
    "\n",
    "HTTP = 'http://'\n",
    "HTTPS = 'https://'\n",
    "ROOT_URL = 'www.dailythanthi.com'\n",
    "\n",
    "\n",
    "CRAWLED_PAGE_COUNT = 0\n",
    "\n",
    "def mkdir(path):\n",
    "    if os.path.exists(path):\n",
    "        return\n",
    "    log.info('creating {}'.format(path))\n",
    "    if os.makedirs(path):\n",
    "        log.info('created {}'.format(path))\n",
    "\n",
    "\n",
    "\n",
    "def url_check(a):\n",
    "    log.debug(a)\n",
    "    if (a.startswith(HTTP + ROOT_URL)\n",
    "        or a.startswith(HTTPS + ROOT_URL)):\n",
    "        log.debug('returning true')\n",
    "        return True\n",
    "\n",
    "\n",
    "def extract_links(LINKS, VISITED_LINKS, soup):\n",
    "    links_ = [a.get('href')\n",
    "              for a in soup.find_all('a', href=True)\n",
    "              if url_check(a.get('href'))]\n",
    "    \n",
    "    LINKS.extend([i for i in links_ if i not in VISITED_LINKS])\n",
    "    LINKS = list(set(LINKS))\n",
    "    return LINKS\n",
    "    \n",
    "\n",
    "LINKS         = [HTTP + ROOT_URL, ]\n",
    "VISITED_LINKS = set()\n",
    "DIRS          = [ROOT_DIR, ARTICLES_DIR, COMMENTS_DIR, ABSTRACTS_DIR]\n",
    "SUBDIRS       = [          ARTICLES_DIR, COMMENTS_DIR, ABSTRACTS_DIR]\n",
    "\n",
    "def initialize_dir_structure():\n",
    "    global LINKS\n",
    "    global VISITED_LINKS\n",
    "    \n",
    "    log.info('creating subdirs')\n",
    "    for d in DIRS:\n",
    "        log.info('creating {}'.format(d))\n",
    "        mkdir(d)\n",
    "\n",
    "    VISITED_LINKS = set()\n",
    "    try:\n",
    "        with open(VISITED_LINKS_FILEPATH, 'r') as f:\n",
    "            VISITED_LINKS = set(f.readlines())\n",
    "    except FileNotFoundError:\n",
    "        open(VISITED_LINKS_FILEPATH, 'w').close()\n",
    "\n",
    "    try:\n",
    "        with open(LINKS_FILEPATH, 'r') as f:\n",
    "            LINKS.extend(list(set(f.readlines())))\n",
    "    except FileNotFoundError:\n",
    "        open(LINKS_FILEPATH, 'w').close()\n",
    "\n",
    "        \n",
    "uid_ = 0\n",
    "name = '{}'.format(uid_)\n",
    "def extract_year_month(page_link, soup):\n",
    "    global uid_\n",
    "    global name\n",
    "    year, month = '0000', '00'\n",
    "    m = re.search('(\\d{4})\\/(\\d{2})/(\\d+)\\/(.*)', page_link)\n",
    "    if m:\n",
    "        log.debug(pformat(m))\n",
    "        year, month, uid, name = m.groups()\n",
    "        for d in SUBDIRS:\n",
    "            mkdir('{}/{}/{}'.format(d, year, month))\n",
    "            \n",
    "    else:\n",
    "        uid_ += 1\n",
    "        uid = uid_\n",
    "        \n",
    "    return year, month, uid, name\n",
    "\n",
    "def process_page(page_name, soup):\n",
    "    global CRAWLED_PAGE_COUNT\n",
    "    \n",
    "    year, month, uid, name = extract_year_month(page_name, soup)\n",
    "    log.info('year, month = {}, {}'.format(year, month))\n",
    "    \n",
    "    # remove all javascript and stylesheet code\n",
    "    for script in soup([\"script\", \"style\"]): \n",
    "        script.extract()\n",
    "\n",
    "    content = soup.find(id='ArticleDetailContent')\n",
    "    if not content:\n",
    "        log.error('content extraction failed')\n",
    "        log.error('{}'.format(page_name))\n",
    "    else:\n",
    "        content = content.text\n",
    "        with open('{}/{}/{}/{}.txt'.format(ARTICLES_DIR, year, month, name), 'w') as f:\n",
    "            f.write('{}\\n------------------\\n'.format(page_name))\n",
    "            f.write(content)\n",
    "\n",
    "        # extract title\n",
    "        title = soup.find('div', class_=['Article_Headline'])\n",
    "        if not title:\n",
    "            log.error('title extraction failed -- {}'.format(page_name))\n",
    "        else:                    \n",
    "            title_p = title.find('p')\n",
    "            if title_p :\n",
    "                title_p = title_p.extract()\n",
    "                title_tamil, title_english = title_p.text.split('+ \"||\" +')\n",
    "\n",
    "                log.info('title := {}'.format(title_tamil))\n",
    "                title_file.write('ta:{}\\n'.format(title_tamil.replace('\\n', '').replace('\\r', '')))\n",
    "                title_file.write('en:{}\\n'.format(title_english.replace('\\n', '').replace('\\r', '')))\n",
    "\n",
    "            else:\n",
    "                title_tamil = title.text\n",
    "                title_file.write('ta:{}\\n'.format(title_tamil.replace('\\n', '').replace('\\r', '')))\n",
    "\n",
    "            title_file.write('\\n\\n')\n",
    "            title_file.flush()\n",
    "\n",
    "        #extract abstract\n",
    "        abstract = soup.find(id='ArticleAbstract')\n",
    "        if not abstract:\n",
    "            log.error('abstract extraction failed')\n",
    "            log.error('{}'.format(page_name))\n",
    "        else:\n",
    "            abstract = abstract.text\n",
    "            with open('{}/{}/{}/{}.txt'.format(ABSTRACTS_DIR, year, month, name), 'w') as f:\n",
    "                f.write('{}\\n------------------\\n'.format(page_name))\n",
    "                f.write(abstract)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        #extract comment\n",
    "        iframes = soup.find('div', id= 'vuukle-comments')\n",
    "        log.debug('IFRAMES: {}'.format(iframes))\n",
    "        if not iframes:\n",
    "            log.error('comment iframe extraction failed')\n",
    "            log.error('== {}'.format(page_name))\n",
    "        else:\n",
    "\n",
    "            comments = iframe.find_all(class_='comment-content')\n",
    "            if not comments:\n",
    "                log.error('comment iframe extraction failed')\n",
    "                log.error('== {}'.format(page_name))\n",
    "            else:\n",
    "                print(comments)\n",
    "                comment = comments.text\n",
    "                with open('{}/{}/{}/{}.txt'.format(COMMENTS_DIR, year, month, name), 'w') as f:\n",
    "                    pass\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        CRAWLED_PAGE_COUNT += 1\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    initialize_dir_structure()\n",
    "    title_file = open(TITLE_LIST_FILEPATH, 'a')\n",
    "    try:\n",
    "        while len(LINKS) > 0 or CRAWLED_PAGE_COUNT < MAX_COUNT:\n",
    "            print('\\n')\n",
    "\n",
    "            current_link = LINKS.pop(0).strip()\n",
    "            if not url_check(current_link):\n",
    "                current_link = HTTP + ROOT_URL + current_link\n",
    "\n",
    "            if current_link not in VISITED_LINKS or len(current_link) < 30:\n",
    "                if CRAWLED_PAGE_COUNT > MAX_COUNT:\n",
    "                    break\n",
    "\n",
    "                VISITED_LINKS.add(current_link)\n",
    "\n",
    "                try:\n",
    "                    page_name = urllib.parse.unquote(current_link)\n",
    "                    log.info('crawl_count: {}'.format(CRAWLED_PAGE_COUNT))\n",
    "                    log.info('processing: {}'.format(page_name))\n",
    "\n",
    "\n",
    "                    # access current link content\n",
    "                    page = requests.get('{}'.format(current_link))\n",
    "                    soup = bs(page.content, 'html.parser')\n",
    "\n",
    "                    # extract links\n",
    "                    LINKS = extract_links(LINKS, VISITED_LINKS, soup)\n",
    "                    log.info('LINKS count:= {}'.format(len(LINKS)))\n",
    "                    log.debug(pformat(LINKS))\n",
    "\n",
    "                    process_page(page_name, soup)\n",
    "\n",
    "                except KeyboardInterrupt:\n",
    "                    with open(VISITED_LINKS_FILEPATH, 'w') as f:\n",
    "                        f.write('\\n'.join(VISITED_LINKS))\n",
    "\n",
    "                    with open(LINKS_FILEPATH, 'w') as f:\n",
    "                        f.write('\\n'.join(LINKS))\n",
    "                        \n",
    "                    raise KeyboardInterrupt\n",
    "                \n",
    "                except:\n",
    "                    log.exception(current_link)\n",
    "                    with open(VISITED_LINKS_FILEPATH, 'w') as f:\n",
    "                        f.write('\\n'.join(VISITED_LINKS))\n",
    "            else:\n",
    "                log.info('already visited {}'.format(urllib.parse.unquote(current_link)))\n",
    "\n",
    "    except:\n",
    "        log.exception('###############')\n",
    "        title_file.close()\n",
    "        with open(VISITED_LINKS_FILEPATH, 'w') as f:\n",
    "            f.write('\\n'.join(VISITED_LINKS))\n",
    "\n",
    "        with open(LINKS_FILEPATH, 'w') as f:\n",
    "            f.write('\\n'.join(LINKS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f448999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
